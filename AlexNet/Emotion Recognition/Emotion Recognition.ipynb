{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spencergoldberg1/Data-Science/blob/develop/Emotion%20Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "LUaVnys1Qyuj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# 2. Load labeled images from folders\n",
        "# data_dir = '/content/gdrive/MyDrive/Data1/antsbeesdataset'\n",
        "data_dir = '/content/gdrive/MyDrive/DataSci/FacialRecognition'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79573198-bcd6-415c-e744-0a2a691b2500",
        "id": "bZ7O0ihLQyuk"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/DataSci/FacialRecognition"
      ],
      "metadata": {
        "id": "EM6GoyVs4S2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68d72d4-e841-423a-e562-a9a7e9f59148"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/DataSci/FacialRecognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-7Wgp5e4U6H",
        "outputId": "0b68eb6a-b9c2-46b0-d38d-4ecf6f1a097b"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalid\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Pre-process the data and create data loaders\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "viMy5KEnQyul"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_datasets = {x: datasets.ImageFolder(data_dir + '/' + x, data_transforms[x]) for x in ['train', 'valid']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=4) for x in ['train', 'valid']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
        "class_names = image_datasets['train'].classes\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "L5uNetpjQyum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c7fed4-7952-46b5-d857-95d7790c2b63"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Set up the AlexNet architecture\n",
        "alexnet = models.alexnet(pretrained=True)"
      ],
      "metadata": {
        "id": "4ygsNg7ZQyum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a4718e-5f9e-4d51-bdeb-f78b2067dd00"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet"
      ],
      "metadata": {
        "id": "oaLKTPxABRwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8bb9ec1-6b8e-4dfa-a375-834ec3029e71"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_ftrs = alexnet.classifier[6].in_features\n",
        "alexnet.classifier[6] = nn.Linear(num_ftrs, len(class_names)) # Change last layer\n",
        "alexnet = alexnet.to(device) # Put on GPU"
      ],
      "metadata": {
        "id": "z6-2nk8_BQTr"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6610ec-8b83-4ef3-b2f4-de2d2019a484",
        "id": "lRtUcENjQyun"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(alexnet, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b917539-7ae0-4f5d-b597-4ccfbfdc12be",
        "id": "q5FX_4lxQyuo"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 55, 55]          23,296\n",
            "              ReLU-2           [-1, 64, 55, 55]               0\n",
            "         MaxPool2d-3           [-1, 64, 27, 27]               0\n",
            "            Conv2d-4          [-1, 192, 27, 27]         307,392\n",
            "              ReLU-5          [-1, 192, 27, 27]               0\n",
            "         MaxPool2d-6          [-1, 192, 13, 13]               0\n",
            "            Conv2d-7          [-1, 384, 13, 13]         663,936\n",
            "              ReLU-8          [-1, 384, 13, 13]               0\n",
            "            Conv2d-9          [-1, 256, 13, 13]         884,992\n",
            "             ReLU-10          [-1, 256, 13, 13]               0\n",
            "           Conv2d-11          [-1, 256, 13, 13]         590,080\n",
            "             ReLU-12          [-1, 256, 13, 13]               0\n",
            "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
            "AdaptiveAvgPool2d-14            [-1, 256, 6, 6]               0\n",
            "          Dropout-15                 [-1, 9216]               0\n",
            "           Linear-16                 [-1, 4096]      37,752,832\n",
            "             ReLU-17                 [-1, 4096]               0\n",
            "          Dropout-18                 [-1, 4096]               0\n",
            "           Linear-19                 [-1, 4096]      16,781,312\n",
            "             ReLU-20                 [-1, 4096]               0\n",
            "           Linear-21                    [-1, 2]           8,194\n",
            "================================================================\n",
            "Total params: 57,012,034\n",
            "Trainable params: 57,012,034\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 8.37\n",
            "Params size (MB): 217.48\n",
            "Estimated Total Size (MB): 226.43\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408c1902-b9fe-4f46-b426-f96f5721715d",
        "id": "37A0yA73Qyuo"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['happy', 'sad']"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_sizes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb624ab-444c-4e75-af32-58507b901191",
        "id": "o-V0ud6kQyup"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 140, 'valid': 80}"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, labels in dataloaders[\"train\"]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)"
      ],
      "metadata": {
        "id": "wY4aR1Mc5jKc"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFR9SGTO5kl3",
        "outputId": "61c45d01-fa60-480e-dc67-19db789051f3"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvht0nWV5wdR",
        "outputId": "ee11d242-a689-4f89-bcc7-f3d2118c9e4f"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12])"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-6DBNL45yFr",
        "outputId": "22a5412b-6416-4075-82fa-6ad986fcd073"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = alexnet(inputs)"
      ],
      "metadata": {
        "id": "Jroxm1Nq57kE"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns23X5bt5-TO",
        "outputId": "aabd0672-67d5-446b-8d3a-940e6b4d5069"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUmgmT_o6Si9",
        "outputId": "ccdce4a3-0dc0-472a-bfd1-788397499532"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0058, -0.5356],\n",
              "        [-0.8431,  0.2807],\n",
              "        [ 0.4250, -0.2933],\n",
              "        [-0.3955, -0.0169],\n",
              "        [-0.5662, -0.4864],\n",
              "        [-0.4258, -0.3830],\n",
              "        [-0.5941, -0.1535],\n",
              "        [-0.5970, -0.4751],\n",
              "        [ 0.1766,  0.3349],\n",
              "        [-0.1596,  0.6075],\n",
              "        [ 0.1528,  0.1119],\n",
              "        [-0.1250, -0.0603]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.max(outputs,1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYKwy0V26lrZ",
        "outputId": "81fbec2d-309e-4aa3-87fc-b16c68c38269"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0058,  0.2807,  0.4250, -0.0169, -0.4864, -0.3830, -0.1535, -0.4751,\n",
              "         0.3349,  0.6075,  0.1528, -0.0603], device='cuda:0',\n",
              "       grad_fn=<MaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.max(outputs,1)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvUQpF4X7GJE",
        "outputId": "e4b35063-6fa7-44e0-9944-8edd70909a11"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = torch.max(outputs, 1)[1]"
      ],
      "metadata": {
        "id": "H69nOoKv6hTR"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCmjTq2Y7OLZ",
        "outputId": "f2f54742-577c-4a0f-8cd9-5d0932b0e5d9"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QI-brXn7QmJ",
        "outputId": "49b8df8b-c395-4fb3-faba-da83ea0e7392"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds == labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dpFhgbR7VSt",
        "outputId": "f8b21cdf-f98d-495f-e8ce-673f58bb8e8a"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False,  True,  True,  True, False,  True,  True,  True, False, False,\n",
              "        False, False], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgfPIPIJ7hz_",
        "outputId": "5a123719-51e9-41b7-bb65-241674db7de2"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(preds == labels)/labels.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVSMVojH7ZGD",
        "outputId": "3365dd20-6241-4723-8283-becb685f7beb"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5000, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the AlexNet model\n",
        "alexnet.train()\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(epoch, \" of \", num_epochs - 1)\n",
        "    print('-' * 10)\n",
        "\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in dataloaders[\"train\"]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        \n",
        "\n",
        "        outputs = alexnet(inputs)\n",
        "        preds = torch.max(outputs, 1)[1]\n",
        "\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    print('Train Acc: {:.4f}'.format(running_corrects / dataset_sizes[\"train\"]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluate the AlexNet model on Validation Data\n",
        "    alexnet.eval()\n",
        "\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in dataloaders[\"valid\"]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "\n",
        "        outputs = alexnet(inputs)\n",
        "        preds = torch.max(outputs, 1)[1]\n",
        "\n",
        "\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    acc_valid = running_corrects / dataset_sizes[\"valid\"]\n",
        "    print('Valid Acc: {:.4f}'.format(acc_valid))\n",
        "    if acc_valid > 0.99:\n",
        "        print(\"Done!\")\n",
        "        break\n",
        "\n",
        "\n",
        "print('Training complete')"
      ],
      "metadata": {
        "id": "BsdI_Z1vM7VM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09c7ed1-07e6-40bc-abbb-b6e08c86e63a"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  of  19\n",
            "----------\n",
            "Train Acc: 0.5000\n",
            "Valid Acc: 0.5500\n",
            "1  of  19\n",
            "----------\n",
            "Train Acc: 0.6500\n",
            "Valid Acc: 0.5875\n",
            "2  of  19\n",
            "----------\n",
            "Train Acc: 0.6214\n",
            "Valid Acc: 0.5875\n",
            "3  of  19\n",
            "----------\n",
            "Train Acc: 0.6571\n",
            "Valid Acc: 0.5375\n",
            "4  of  19\n",
            "----------\n",
            "Train Acc: 0.7071\n",
            "Valid Acc: 0.5500\n",
            "5  of  19\n",
            "----------\n",
            "Train Acc: 0.8500\n",
            "Valid Acc: 0.5500\n",
            "6  of  19\n",
            "----------\n",
            "Train Acc: 0.8214\n",
            "Valid Acc: 0.5250\n",
            "7  of  19\n",
            "----------\n",
            "Train Acc: 0.7429\n",
            "Valid Acc: 0.5250\n",
            "8  of  19\n",
            "----------\n",
            "Train Acc: 0.8571\n",
            "Valid Acc: 0.5375\n",
            "9  of  19\n",
            "----------\n",
            "Train Acc: 0.8571\n",
            "Valid Acc: 0.5250\n",
            "10  of  19\n",
            "----------\n",
            "Train Acc: 0.8714\n",
            "Valid Acc: 0.5625\n",
            "11  of  19\n",
            "----------\n",
            "Train Acc: 0.8571\n",
            "Valid Acc: 0.5625\n",
            "12  of  19\n",
            "----------\n",
            "Train Acc: 0.8857\n",
            "Valid Acc: 0.5500\n",
            "13  of  19\n",
            "----------\n",
            "Train Acc: 0.8643\n",
            "Valid Acc: 0.5625\n",
            "14  of  19\n",
            "----------\n",
            "Train Acc: 0.8714\n",
            "Valid Acc: 0.6500\n",
            "15  of  19\n",
            "----------\n",
            "Train Acc: 0.9071\n",
            "Valid Acc: 0.6250\n",
            "16  of  19\n",
            "----------\n",
            "Train Acc: 0.9071\n",
            "Valid Acc: 0.6250\n",
            "17  of  19\n",
            "----------\n",
            "Train Acc: 0.9500\n",
            "Valid Acc: 0.5750\n",
            "18  of  19\n",
            "----------\n",
            "Train Acc: 0.9143\n",
            "Valid Acc: 0.5750\n",
            "19  of  19\n",
            "----------\n",
            "Train Acc: 0.9357\n",
            "Valid Acc: 0.6625\n",
            "Training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SCVeSlOF-CjA",
        "outputId": "e796fe5f-9642-4168-bf2b-cafe044f0d03"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/DataSci/FacialRecognition'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/gdrive/MyDrive/DataSci/FacialRecognition/alexnet_classification_emotions.pth'"
      ],
      "metadata": {
        "id": "zmhjefa5QdSu"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(alexnet.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "dBlJWh3QQyur"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r73P4wWv-Qj5",
        "outputId": "220282de-48e7-455b-d6b7-680dbff44baf"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alexnet_classification_emotions.pth  \u001b[0m\u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalid\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import imageio as io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load the trained AlexNet model\n",
        "def load_model(model_path):\n",
        "    model = models.alexnet()\n",
        "    num_ftrs = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(num_ftrs, len(class_names))\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    return model.to(device)\n",
        "\n",
        "# 2. Define a function to load an image from a URL and preprocess it\n",
        "def preprocess_image(url, transform):\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    img_tensor = transform(img)\n",
        "    return img_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "# 3. Perform inference using the loaded model\n",
        "def predict_image_url(url, model):\n",
        "    img_tensor = preprocess_image(url, data_transforms['valid'])\n",
        "    output = model(img_tensor)\n",
        "    pred = torch.max(output, 1)[1]\n",
        "    return class_names[pred]"
      ],
      "metadata": {
        "id": "6owtKjwZQyus"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = load_model(model_path)"
      ],
      "metadata": {
        "id": "a51GZ4wW-4yw"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(x):\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(x, cmap = 'gray')\n",
        "    ax.axis('off')\n",
        "    fig.set_size_inches(18, 10)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TC1vp5Q5jCSQ"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = ['https://personalexcellence.co/files/girl-smiling2.jpg', 'https://www.allprodad.com/wp-content/uploads/2021/03/05-12-21-happy-people.jpg', 'https://previews.123rf.com/images/bowie15/bowie151401/bowie15140100080/39843138-sad-man.jpg', 'https://st.depositphotos.com/2069237/2523/i/600/depositphotos_25235121-stock-photo-sad-crying-disappointed-funny-business.jpg', 'https://cdn.tinybuddha.com/wp-content/uploads/2016/01/Happy-Guy.jpg']"
      ],
      "metadata": {
        "id": "HxRmKBwQQyut"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_url in urls:\n",
        "  prediction = predict_image_url(image_url, trained_model)\n",
        "  print(\"The predicted class for the input image is:\", prediction)"
      ],
      "metadata": {
        "id": "8fTxsPSoSi6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe777bce-982f-4daa-a65e-c401ed270c77"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted class for the input image is: happy\n",
            "The predicted class for the input image is: happy\n",
            "The predicted class for the input image is: sad\n",
            "The predicted class for the input image is: sad\n",
            "The predicted class for the input image is: happy\n"
          ]
        }
      ]
    }
  ]
}